import torch.nn as nn
import math
import warnings
import torch

def _no_grad_trunc_normal_(tensor, mean, std, a, b):
    # Cut & paste from PyTorch official master until it's in a few official releases - RW
    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf
    def norm_cdf(x):  # pragma: no cover
        # Computes standard normal cumulative distribution function
        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0

    if (mean < a - 2 * std) or (mean > b + 2 * std):
        warnings.warn(
            "mean is more than 2 std from [a, b] in nn.init.trunc_normal_. "
            "The distribution of values may be incorrect.",
            stacklevel=2,
        )

    with torch.no_grad():
        # Values are generated by using a truncated uniform distribution and
        # then using the inverse CDF for the normal distribution.
        # Get upper and lower cdf values
        l = norm_cdf((a - mean) / std)
        u = norm_cdf((b - mean) / std)

        # Uniformly fill tensor with values from [l, u], then translate to
        # [2l-1, 2u-1].
        tensor.uniform_(2 * l - 1, 2 * u - 1)

        # Use inverse cdf transform for normal distribution to get truncated
        # standard normal
        tensor.erfinv_()

        # Transform to proper mean, std
        tensor.mul_(std * math.sqrt(2.0))
        tensor.add_(mean)

        # Clamp to ensure it's in the proper range
        tensor.clamp_(min=a, max=b)
        return tensor

class Head(nn.Module):
    def __init__(self, n_embed, n_classes, n_hidden=512, n_bottleneck=256, temp=0.1):
        super().__init__()

        self.temp = temp
        self.body = nn.Sequential(
            nn.Linear(n_embed, n_hidden),
            nn.GELU(),
            nn.Linear(n_hidden, n_bottleneck)
        )
        self.apply(self._init_weights)
        self.last_linear = nn.utils.weight_norm(nn.Linear(n_bottleneck, n_classes, bias=False))
        self.last_linear.weight_g.data.fill_(1)
        self.softmax = nn.Softmax(dim=1)

    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            _no_grad_trunc_normal_(m.weight, mean=0, std=.02, a=-2, b=2)
        if isinstance(m, nn.Linear) and m.bias is not None:
            nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        logits = self.last_linear(self.body(x))
        logits /= self.temp
        return self.softmax(logits)

class MultiHead(nn.Module):
    def __init__(self, n_heads, n_embed, n_classes, n_hidden=512):
        super().__init__()
        head_kwargs = {
            "n_embed": n_embed, 
            "n_classes": n_classes, 
            "n_hidden": n_hidden
        }
        self.heads = nn.ModuleList([Head(**head_kwargs) for _ in range(n_heads)])

    def forward(self, x):
        return [head(x) for head in self.heads]