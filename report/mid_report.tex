\documentclass[a4paper,12pt]{extarticle}
\usepackage{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{graphicx}
\usepackage{colortbl}
\usepackage{tikz}
\usepackage{pgf}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{indentfirst}
\usepackage[
backend=biber,
style=numeric,
maxbibnames=99
]{biblatex}
\addbibresource{refs.bib}
\usepackage[colorlinks,citecolor=blue,linkcolor=blue,bookmarks=false,hypertexnames=true, urlcolor=blue]{hyperref} 
\usepackage{indentfirst}
\usepackage{mathtools}
\usepackage{booktabs}
\usepackage[flushleft]{threeparttable}
\usepackage{tablefootnote}

\usepackage{chngcntr} % нумерация графиков и таблиц по секциям
\counterwithin{table}{section}
\counterwithin{figure}{section}

\graphicspath{{graphics/}}%путь к рисункам

\makeatletter
% \renewcommand{\@biblabel}[1]{#1.} % Заменяем библиографию с квадратных скобок на точку:
\makeatother

\geometry{left=2.5cm}% левое поле
\geometry{right=1.0cm}% правое поле
\geometry{top=2.0cm}% верхнее поле
\geometry{bottom=2.0cm}% нижнее поле
\setlength{\parindent}{1.25cm}
\renewcommand{\baselinestretch}{1.5} % междустрочный интервал


\newcommand{\bibref}[3]{\hyperlink{#1}{#2 (#3)}} % biblabel, authors, year
\addto\captionsrussian{\def\refname{Список литературы (или источников)}} 

\renewcommand{\theenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumi}{\arabic{enumi}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumii}{.\arabic{enumii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumii}{\arabic{enumi}.\arabic{enumii}.}% Меняем везде перечисления на цифра.цифра
\renewcommand{\theenumiii}{.\arabic{enumiii}}% Меняем везде перечисления на цифра.цифра
\renewcommand{\labelenumiii}{\arabic{enumi}.\arabic{enumii}.\arabic{enumiii}.}% Меняем везде перечисления на цифра.цифра

\begin{document}
\input{title}% это титульный лист - выберите подходящий вам из имеющихся в проекте вариантов
\newpage
\setcounter{page}{2}

{
	\hypersetup{linkcolor=black}
	\tableofcontents
}

\newpage

\newpage
\section*{Аннотация}   % this is how to use russian
    Существующие методы глубинного обучения для кластеризации 
    изображений работают довольно хорошо. Однако вопрос качественной
    кластеризации аудио остается открытым. В этой работе мы планируем
    адаптировать лучшие методы кластеризации изображений для 
    задачи кластеризации аудио.

\addcontentsline{toc}{section}{Аннотация}

\section*{Ключевые слова}
Глубинное обучение, обучение без учителя, кластеризация
\pagebreak

\section{Введение}

\subsection{Постановка задачи}

В задаче классификации мы имеем обучающую выборку, где для каждого объекта
известен его класс и от нас требуется моделировать распределение
классов на пространстве объектов. Задача кластеризации более сложная
-- необходимо разбить объекты на осмысленные группы не зная ни 
самих групп, ни их распределения.

\subsection{Метрики качества}

Чтобы замерить качество кластеризации как правило метод
решает задачу на размеченном для кластеризации датасете, 
разумеется не используя метки классов. Полученные после 
кластеризации номера кластеров будем называть \textit{псевдометками}.
Псевдометки сравниваются с настоящими метками
и качество определяется как некоторый вид корреляции между ними.

Одной из самых популярных метрик качества является NMI(Normalized Mutual Information).
Пусть у нас есть пара случайных величин $X$ и $Y$, тогда:

\[
	\text{NMI}(X,Y) = \frac{\text{KL}(p_{(X, Y)}| p_X \otimes p_Y)}{\sqrt{H(X)H(Y)}}
\]
При подсчете NMI мы считаем распределения меток и псевдометок 
за $X$ и $Y$ и вычисляем \textit{оценку максимального правдоподобия}\footnote{далее ОМП}. NMI 
принимает значения от 0 до 1.

Еще одной используемой метрикой является ARI(Adjusted Rand Index).
Это скорректированная версия метрики RI\footnote{\url{https://en.wikipedia.org/wiki/Rand_index}} (Rand Index).
Определяется ARI следующим образом:

\[
	\text{ARI}(X, Y) = \frac{\text{RI}(X, Y) - \text{E}[\text{RI}(X, Y)]}{1 - \text{E}[\text{RI}(X, Y)]}
\]

ARI принимает значения от 0 до 1.

Также в качестве метрики качества можно использовать долю 
правильных ответов как и в задаче классификации. Однако предварительно 
необходимо решить \textit{задачу о назначениях}\footnote{\url{https://en.wikipedia.org/wiki/Assignment_problem}}.
между псевдометками и метками.
Мы переименуем псевдометки так, чтобы достичь максимального качества.
Затем на переименованных псевдометках мы посчитаем долю правильных ответов, которую 
и используем в качестве метрики качества.

\subsection{Классические методы решения}

С точки зрения классических методов кластеризуемые
объекты это точки в многомерном пространстве. Такие
методы обычно имеют итерационную природу и решают 
задачу выделения кластеров точек находя их скопления, 
области с повышенной плотностью, некоторые структуры.
Посмотрим на несколько примеров.

\subsubsection{K-means}

Одним из самых простых и известных методов является
K-means. Он работает на базе EM-алгоритма\footnote{\url{https://en.wikipedia.org/wiki/Expectation\%E2\%80\%93maximization_algorithm}}.
K-means итерационно пытается найти два неизвестных набора переменных ---
номера кластеров для объектов и центры этих кластеров. 
Количество кластеров фиксировано и задается до начала 
работы алгоритма в качестве гиперпараметра $k$.

В начале своей работы классический K-means инициализирует все 
центры кластеров случайно. Затем чередуются E и M шаги до 
сходимости. На E-шаге мы назначаем каждому объекту номер
кластера к центру которого объект ближе всего в качестве псевдометки.
На M-шаге мы вычисляем ОМП для каждого центра кластера, то есть 
берем в качестве центра кластера усредненную точку из всех объектов 
с псевдометкой этого кластера.

\subsubsection{DBSCAN}

\subsubsection{MeanShift}

\subsubsection{AgglomerativeClustering}

\section{Обзор литературы}

При кластеризации сложных объектов таких как изображения или 
аудио недостаточно вытянуть данные объекта в вектор и применить
классический алгоритм кластеризации для полученных точек. 
Чтобы алгоритмы кластеризации хорошо работали входное 
пространство точек должно обладать некоторыми свойствами.
В идеале близкие в этом пространстве точки должны принадлежать 
к одной группе.

Таким образом задачу кластеризации сложных объектов можно 
разделить на две части -- получение представлений объектов 
образующих пригодное для кластеризации пространство точек и 
сама кластеризация этих представлений. Метод переводящий объекты
в признаки будем называть \textit{энкодером}.

\subsection{DeepCluster}

Одним из первых методов использующих глубинное обучения для 
кластеризации изображений был
DeepCluster \cite{Caron_2018_ECCV}. DeepCluster использует 
сверточную нейросеть $f_\theta$ в качестве энкодера.
К полученным признакам применяется K-means и мы получаем
псевдометку $y_n$ для каждого изображения $x_n$. Затем к 
сверточной сети прикрепляется
классификационная голова $g_W$. $g_W$ предсказывает 
вероятности кластеров для объекта по его признакам 
полученным из $f_\theta$. Псевдометки используются как настоящие для подсчета логистической 
функции потерь. Таким образом мы решаем следующую задачу 
оптимизации:

\[
	\min_{\theta, W} \frac{1}{N}\sum_{n=1}^N 
	l(g_W(f_\theta(x_n)), y_n)
\]

Здесь $N$ -- это размер батча, $l$ логистическая 
мультиномиальная функция потерь

По усредненному значению функции потерь для батча делается проход назад и веса 
$\theta$ и $W$ обновляются стохастическим градиентным спуском. Затем 
шаги кластеризации и оптимизации параметров сети повторяются 
заданное количество эпох.

Однако в такой постановке метод выраждается в тривиальные 
решения. Их существует 2 типа - пустые кластеры и 
тривиальная параметризация

Чтобы не допустить возникновения пустых кластеров 
сделаем следующую модификацию K-means. Пусть 
на некоторой итерации у нас появился пустой 
класс A. Тогда возьмем случайный непустой класс 
B. Добавим к центру B шум получив новый центр 
для кластера A. Переназначим классы точек из B 
так чтобы каждая точка имела класс ближайшего 
центра кластера. 

Тривиальная параметризация возникает когда 
распределение классов становится сильно неравномерным. 
Тогда классификационной голове становится выгодно выдавать 
только несколько наиболее часто встречающихся классов игнорируя остальные. 
Чтобы этого избежать необходимо сэмплировать объекты для батча 
из равномерного распределения по псевдометкам с предыдущего шага.

\subsection{SPICE}

SPICE предложенный в \cite{niu2021spice}, является


\section{План дальнейшей работы}

\section{Примеры} 
\subsection{Ссылки на статьи}

Ссылки на статьи оформляются с помощью пакета \texttt{biblatex}, например~\cite{chirkova18}. В описании статье в bib файле нужно обязательно указывать место публикации работы (журнал или конференцию) и год. Обратите внимание, что для описания статей из разных источников в списке литературы используются разные команды в bib файле: статья из журнала~\cite{ctan}, статья с конференции~\cite{chirkova18}, книга~\cite{knuth-acp}, глава книги~\cite{knuth-fa}. Если статься еще не опубликована нигде, а только выложена на arXiv, то на нее тоже можно сослаться~\cite{chirkova18_arxiv}, но предпочтительно ссылаться на опубликованную версию, если она уже существует. Если вы хотите сослаться на сайт, то можно либо так же внести его в список литературы~\cite{knuthwebsite} (рекомендуется, если таких ссылок у вас много из-за особенностей темы вашего проекта), либо использовать ссылку внизу страницы\footnote{Книги доступны по ссылке: \url{http://www-cs-faculty.stanford.edu/~uno/abcde.html}, дата обр. 16.05.2013}. При работе с онлайн ресурсами не забывайте указывать дату обращения к этому ресурсу, так как в отличие от опубликованных статей, эти ресурсы могут измениться в любой момент.

\subsection{Рисунки}

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.8\textwidth]{example.png}
	\caption{Пример графика. Тут должна быть подпись, поясняющая что происходит на рисунке (краткая, но достаточная для понимания основной идеи графика).}
	\label{fig:by_epochs}
\end{figure}

Все рисунки в тексте должны иметь подписи и вы на них должны ссылаться в тексте. Например, на Рисунке~\ref{fig:by_epochs} изображен пример графика. Не забывайте подписывать все оси на графиках, добавлять легенду и пояснять все обозначения, а также используйте адекватного размера шрифты и толщину линий на графиках (все должно быть видно и понятно без многократного увеличения). На рисунке из примера явно не хватает обозначения синей линии в легенде.


\subsection{Таблицы}

Все таблицы в тексте тоже должны иметь подписи и вы на них должны ссылаться в тексте. Например, в Таблице~\ref{table:long_epochs} показаны результаты примерного эксперимента. 


\begin{table}[ht]
	\caption{Пример таблички. Тут должна быть подпись, поясняющая что происходит в таблице (краткая, но по делу).}
	\label{table:long_epochs}
	\footnotesize
	\centering
	\begin{tabular}{lrrrrrrrr}
		\toprule
		& \multicolumn{3}{c}{$\mathsf{Val}$} &
		\multicolumn{3}{c}{$\mathsf{Test}$} \\
		\cmidrule(lr){2-4} \cmidrule(l){5-7} 
		{} &  $\mathsf{Prec}$ &  $\mathsf{Rec}$ &  $\mathsf{F1}$ &  $\mathsf{Prec}$ &  $\mathsf{Rec}$ &  $\mathsf{F1}$  &  $\mathsf{nodes}$ & $\mathsf{subtokens}$\\
		\midrule
		запуск 1    &    0.4894 &   0.3775 &  0.4263 &     0.4824 &    0.3683 &   0.4177 & 10029 & 179\\
		запуск 2    &    0.4887 &   0.3739 &  0.4237 &     0.4891 &    0.3724 &   0.4228 & 10039 & 177\\
		запуск 3    &    0.4820 &   0.3751 &  0.4219 &     0.4838 &    0.3677 &   0.4178 & 10037&	180\\
		\midrule
		\bf{среднее} &    \bf{0.4867} &   \bf{0.3755} &  \bf{0.4239} &    \bf{ 0.4851} &    \bf{0.3695} &   \bf{0.4195} \\
		\bf{дисперсия}  &    0.0041 &   0.0019 &  0.0022 &     0.0036 &    0.0025 &   0.0029 \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{Формулы}

Формулы стоит центрировать, а также нумеровать, если вы ссылаете на них в тексте. Также не забывайте пояснять все обозначения в формулах. Например, запишем следующую задачу оптимизации:
\begin{equation}
    \label{eq:si_opt}
        \theta* = \min_{\theta} F(\theta),
\end{equation}
где $F$~-- квадратичная функция от параметра $\theta$. При необходимости, далее в тексте можно сослаться на формулу~(\ref{eq:si_opt}). При этом, в зависимости от конкретных формул, можно использовать разные слова: формула, уравнение, задача оптимизации и т.п.


	
\newpage 
\printbibliography[heading=bibintoc] 

% \begin{thebibliography}{0}
% 	\bibitem{chirkova18}\hypertarget{chirkova18}{}
% 	\href{https://arxiv.org/abs/1810.10927}
% 	{Nadezhda Chirkova, Ekaterina Lobacheva, Dmitry Vetrov. Bayesian Compression for Natural Language Processing. In EMNLP 2018.}
% \end{thebibliography}
	
	
\end{document}
